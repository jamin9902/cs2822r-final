{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets[vision]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing ImageNet from https://huggingface.co/datasets/ILSVRC/imagenet-1k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip uninstall huggingface_hub datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install huggingface_hub datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login with Hugging face credentials (need to configure with access token)\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "# Stream the dataset instead of downloading the whole thing\n",
    "ds = load_dataset(\"imagenet-1k\", streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code below is the intial code to extract the images and print the labels as is, no perturbations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = ds['train']\n",
    "# train_ds_iter = iter(train_ds)\n",
    "\n",
    "# # Function to display a few images and their labels\n",
    "# def preview_images(ds_iter, num_samples=5):\n",
    "#     for _ in range(num_samples):\n",
    "#         # Get the next sample\n",
    "#         sample = next(ds_iter)\n",
    "        \n",
    "#         # Extract the image and label\n",
    "#         img = sample['image']  # This is already a PIL image\n",
    "#         label = sample['label']\n",
    "        \n",
    "#         # Display the image\n",
    "#         img.show()  # This will open the image in the default image viewer\n",
    "        \n",
    "#         # Print the label\n",
    "#         print(f\"Label: {label}\")\n",
    "\n",
    "# # Preview the first 5 images and their labels\n",
    "# preview_images(train_ds_iter, num_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code below hosts three functions, then prints the resulting images plus the perturbations for 5 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install \"numpy<2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from transformers import AutoFeatureExtractor, AutoModelForImageClassification\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the pretrained ResNet model and feature extractor\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/resnet-50\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\"microsoft/resnet-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to Print out the original, un-edited images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from lime import lime_image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from captum.attr import Saliency\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to dynamically save images with specified names\n",
    "def save_image(image, name, count):\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'image_{name}_{count}.png', bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "# Function to generate 10 images per sample\n",
    "def generate_images(image, count):\n",
    "    # Save original unperturbed image\n",
    "    save_image(image, 'original', count)\n",
    "    \n",
    "    # Noise Injection with different degrees of perturbation\n",
    "    stddev_values = [5, 25, 50]  # Small, Moderate, Significant\n",
    "    for i, stddev in enumerate(stddev_values, 1):\n",
    "        noisy_image = noise_injection(image, stddev=stddev)\n",
    "        save_image(noisy_image, f'noise_{stddev}', count)\n",
    "    \n",
    "    # Occlusion with different percentages\n",
    "    occlusion_values = [0.1, 0.3, 0.5]  # Small, Moderate, Significant occlusion\n",
    "    for i, percentage in enumerate(occlusion_values, 1):\n",
    "        occluded_image = occlusion(image, percentage=percentage)\n",
    "        save_image(occluded_image, f'occlusion_{percentage}', count)\n",
    "    \n",
    "    # Resolution Reduction with different block sizes\n",
    "    block_sizes = [5, 10, 20]  # Small, Moderate, Significant reduction\n",
    "    for i, block_size in enumerate(block_sizes, 1):\n",
    "        reduced_image = resolution_reduction(image, block_size=block_size)\n",
    "        save_image(reduced_image, f'resolution_{block_size}', count)\n",
    "\n",
    "# Process 6 images from the dataset\n",
    "for count in range(6):\n",
    "    sample = next(train_ds_iter)\n",
    "    img = sample['image']  # PIL image\n",
    "    generate_images(img, count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to Print out the Explanation Methods Per Image, Per Perturbation Method and its Varying Degrees of Severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from lime import lime_image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from captum.attr import Saliency\n",
    "\n",
    "# Perturbation functions\n",
    "def noise_injection(image, distribution='gaussian', mean=0, stddev=25):\n",
    "    img_array = np.array(image)\n",
    "    noise = np.random.normal(mean, stddev, img_array.shape) if distribution == 'gaussian' else np.random.uniform(-stddev, stddev, img_array.shape)\n",
    "    noisy_image = np.clip(img_array + noise, 0, 255).astype(np.uint8)\n",
    "    return Image.fromarray(noisy_image)\n",
    "\n",
    "def occlusion(image, percentage=0.2):\n",
    "    img_array = np.array(image)\n",
    "    height, width, _ = img_array.shape\n",
    "    occ_height = int(height * percentage)\n",
    "    occ_width = int(width * percentage)\n",
    "    top_left_x = np.random.randint(0, width - occ_width)\n",
    "    top_left_y = np.random.randint(0, height - occ_height)\n",
    "    img_array[top_left_y:top_left_y + occ_height, top_left_x:top_left_x + occ_width] = 0\n",
    "    return Image.fromarray(img_array)\n",
    "\n",
    "def resolution_reduction(image, block_size=2):\n",
    "    img_array = np.array(image)\n",
    "    height, width, _ = img_array.shape\n",
    "    for i in range(0, height, block_size):\n",
    "        for j in range(0, width, block_size):\n",
    "            block = img_array[i:min(i+block_size, height), j:min(j+block_size, width)]\n",
    "            avg_color = block.mean(axis=(0, 1)).astype(int)\n",
    "            img_array[i:min(i+block_size, height), j:min(j+block_size, width)] = avg_color\n",
    "    return Image.fromarray(img_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to Compute and Run Smoothgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SmoothGrad\n",
    "def compute_smoothgrad(inputs, model, target_class_idx, n_samples=50, stdev_spread=0.15):\n",
    "    image_tensor = inputs['pixel_values'].clone().detach().requires_grad_(True)\n",
    "    saliency = Saliency(lambda noisy_image: model(noisy_image).logits)\n",
    "    stdev = stdev_spread * (image_tensor.max() - image_tensor.min()).item()\n",
    "    smooth_grad = torch.zeros_like(image_tensor)\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        noise = torch.randn_like(image_tensor) * stdev\n",
    "        noisy_image = image_tensor + noise\n",
    "        noisy_saliency = saliency.attribute(noisy_image, target=target_class_idx)\n",
    "        smooth_grad += noisy_saliency\n",
    "    \n",
    "    smooth_grad /= n_samples\n",
    "    smooth_grad = smooth_grad.squeeze().detach().numpy()\n",
    "    return smooth_grad\n",
    "\n",
    "def visualize_smoothgrad(smooth_grad_tensor, image, filename):\n",
    "    smooth_grad = smooth_grad_tensor.squeeze()\n",
    "    if smooth_grad.ndim == 3 and smooth_grad.shape[0] == 3:\n",
    "        smooth_grad = np.mean(smooth_grad, axis=0)\n",
    "    smooth_grad = (smooth_grad - smooth_grad.min()) / (smooth_grad.max() - smooth_grad.min())\n",
    "    smooth_grad = np.uint8(255 * smooth_grad)\n",
    "    heatmap = cv2.applyColorMap(smooth_grad, cv2.COLORMAP_JET)\n",
    "    original_image = np.array(image)\n",
    "    if original_image.ndim == 2:\n",
    "        original_image = cv2.cvtColor(original_image, cv2.COLOR_GRAY2RGB)\n",
    "    heatmap = cv2.resize(heatmap, (original_image.shape[1], original_image.shape[0]))\n",
    "    superimposed_image = cv2.addWeighted(original_image, 0.6, heatmap, 0.4, 0)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(superimposed_image)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to Run LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIME\n",
    "def create_lime_explainer(image, model, feature_extractor):\n",
    "    explainer = lime_image.LimeImageExplainer()\n",
    "\n",
    "    def predict_fn(images):\n",
    "        inputs = feature_extractor(images=[Image.fromarray(img) for img in images], return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        return torch.softmax(outputs.logits, dim=1).cpu().numpy()\n",
    "\n",
    "    # Explain the instance (image) using LIME\n",
    "    explanation = explainer.explain_instance(\n",
    "        np.array(image), predict_fn, top_labels=1, hide_color=0, num_samples=50)\n",
    "    return explanation\n",
    "\n",
    "def visualize_lime_explanation(image, explanation, filename):\n",
    "    # Get the image with the mask and apply the explanation mask (positive_only=False will show all regions)\n",
    "    temp, mask = explanation.get_image_and_mask(\n",
    "        explanation.top_labels[0], positive_only=False, num_features=5, hide_rest=False\n",
    "    )\n",
    "\n",
    "    # Normalize the mask to [0, 1] for applying a colormap\n",
    "    mask = (mask - mask.min()) / (mask.max() - mask.min())\n",
    "    \n",
    "    # Convert the mask to 8-bit values between 0 and 255\n",
    "    mask = np.uint8(255 * mask)\n",
    "\n",
    "    # Apply a color map (e.g., COLORMAP_JET or COLORMAP_SUMMER)\n",
    "    heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
    "\n",
    "    # Convert the original image to a NumPy array\n",
    "    original_image = np.array(image)\n",
    "    if original_image.ndim == 2:  # If grayscale, convert to RGB\n",
    "        original_image = cv2.cvtColor(original_image, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    # Resize the heatmap to match the original image size\n",
    "    heatmap = cv2.resize(heatmap, (original_image.shape[1], original_image.shape[0]))\n",
    "\n",
    "    # Superimpose the heatmap on the original image\n",
    "    superimposed_image = cv2.addWeighted(original_image, 0.6, heatmap, 0.4, 0)\n",
    "\n",
    "    # Save the final result\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(superimposed_image)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to Run the Above Code and Print out Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save function\n",
    "def save_image(image, prefix, img_index, perturbation_type, level):\n",
    "    filename = f\"{prefix}_{img_index}_{perturbation_type}_level_{level}.png\"\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "# Function to generate explanations for the original and perturbed images\n",
    "def generate_images_and_explanations(image, model, feature_extractor, img_index):\n",
    "    perturbations = {\n",
    "        \"noise\": [noise_injection(image, stddev=stddev) for stddev in [10, 25, 50]],\n",
    "        \"occlusion\": [occlusion(image, percentage=percentage) for percentage in [0.1, 0.3, 0.5]],\n",
    "        \"resolution\": [resolution_reduction(image, block_size=block_size) for block_size in [5, 10, 20]],\n",
    "    }\n",
    "\n",
    "    # Original Image\n",
    "    save_image(image, 'original', img_index, 'none', 'original')\n",
    "\n",
    "    # SmoothGrad and LIME for original\n",
    "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "    smooth_grad_explanation = compute_smoothgrad(inputs, model, target_class_idx=0)\n",
    "    visualize_smoothgrad(smooth_grad_explanation, image, f\"smoothgrad_original_{img_index}.png\")\n",
    "    \n",
    "    explanation = create_lime_explainer(image, model, feature_extractor)\n",
    "    visualize_lime_explanation(image, explanation, f\"lime_original_{img_index}.png\")\n",
    "\n",
    "    # Perturbed Images\n",
    "    for perturbation_type, images in perturbations.items():\n",
    "        for level, perturbed_image in enumerate(images, 1):\n",
    "            save_image(perturbed_image, 'perturbed', img_index, perturbation_type, level)\n",
    "\n",
    "            # SmoothGrad for perturbed\n",
    "            inputs_perturbed = feature_extractor(images=perturbed_image, return_tensors=\"pt\")\n",
    "            smooth_grad_explanation = compute_smoothgrad(inputs_perturbed, model, target_class_idx=0)\n",
    "            visualize_smoothgrad(smooth_grad_explanation, perturbed_image, f\"smoothgrad_{perturbation_type}_{img_index}_level_{level}.png\")\n",
    "\n",
    "            # LIME for perturbed\n",
    "            explanation = create_lime_explainer(perturbed_image, model, feature_extractor)\n",
    "            visualize_lime_explanation(perturbed_image, explanation, f\"lime_{perturbation_type}_{img_index}_level_{level}.png\")\n",
    "\n",
    "# Stream the dataset and test explanations\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Stream the dataset\n",
    "ds = load_dataset(\"imagenet-1k\", streaming=True)\n",
    "train_ds = ds['train']\n",
    "\n",
    "# Initialize iterator for the 'train' split\n",
    "train_ds_iter = iter(train_ds)\n",
    "\n",
    "# Test LIME and SmoothGrad explanations on 6 images\n",
    "for img_index in range(6):\n",
    "    sample = next(train_ds_iter)\n",
    "    img = sample['image']  # PIL image\n",
    "    generate_images_and_explanations(img, model, feature_extractor, img_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
